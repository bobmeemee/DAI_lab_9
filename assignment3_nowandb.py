# -*- coding: utf-8 -*-
"""assignment3_nowandb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p2bCCV-Rt_4mqJWsYF2tDGyAZmr7NMGk
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torchvision
import torch.optim as optim

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(device)

class Net(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64,
                               kernel_size=(11,11) ,padding=2, stride=(4,4))
        self.pool = nn.MaxPool2d(kernel_size=3,stride=2)
        self.conv2 = nn.Conv2d(64, 192, kernel_size=(5,5), padding=2)
        self.conv3 = nn.Conv2d(192, 384, kernel_size=(3,3), padding=1)
        self.conv4 = nn.Conv2d(384, 256, kernel_size=(3,3), padding=1)
        self.conv5 = nn.Conv2d(256, 256, kernel_size=(3,3), padding=1)

        self.drop = nn.Dropout(p=0.5)
        self.fc1 = nn.Linear(3 * 3 * 256, 4096)  # 3*3 from kernel dimension
        self.fc2 = nn.Linear(4096, 4096)
        self.fc3 = nn.Linear(4096, num_classes)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x))) # [8,8]
        x = self.pool(F.relu(self.conv2(x)))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = self.pool(F.relu(self.conv5(x)))

        x = torch.flatten(x,1 )
        x = F.relu(self.fc1(self.drop(x)))
        x = F.relu(self.fc2(self.drop(x)))
        x = F.softmax(self.fc3(x), -1)
        return x

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

net = Net(num_classes=len(classes))
net.to(device)

def train(network = None):
    trainloader, testloader = build_dataset(128)
    netw = network
    optimizer = optim.Adam(netw.parameters(),
                               lr=0.001)

    for epoch in range(10):
        avg_loss = train_epoch(netw, trainloader, optimizer)
        accuracy = test(netw, testloader)
        print("Accuracy for epoch " + str(epoch +1) + " : " + str(accuracy))
        print("Test loss for epoch " + str(epoch +1) + " : " + str(avg_loss))

def build_dataset(batch_size):
    transform = transforms.Compose(
        [transforms.ToTensor(),
         transforms.Resize(512),
         transforms.RandomCrop(128),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)
    
    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                         download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)
    
    return trainloader, testloader


def train_epoch(network, loader, optimizer):
    cumu_loss = 0
    running_loss = 0
    criterion = nn.CrossEntropyLoss()
    i = 0
    for _, (data, target) in enumerate(loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()

        # ➡ Forward pass
        loss = criterion(network(data), target)
        cumu_loss += loss.item()
        running_loss += loss.item()

        # ⬅ Backward pass + weight update
        loss.backward()
        optimizer.step()
        i+=1
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print("Running loss for minibatch " + str(i) + ": " + str(running_loss/2000) )
            running_loss = 0.0

    return cumu_loss / len(loader)


def test(network, loader):
    # prepare to count predictions for each class
    correct_pred = {classname: 0 for classname in classes}
    total_pred = {classname: 0 for classname in classes}
    
    # again no gradients needed
    with torch.no_grad():
        for _, (data, target) in enumerate(loader):
            data, labels = data.to(device), target.to(device)
            outputs = network(data)
            _, predictions = torch.max(outputs, 1)
            # collect the correct predictions for each class
            for label, prediction in zip(labels, predictions):
                if label == prediction:
                    correct_pred[classes[label]] += 1
                total_pred[classes[label]] += 1

    # log accuracy for each class
    tot_accuracy = 0
    weight = float(1/len(classes))
    print(weight)
    for classname, correct_count in correct_pred.items():
        accuracy = 100 * float(correct_count) / total_pred[classname]
        print(classname + ": " + str(accuracy))
        tot_accuracy += float(accuracy* weight)
    return tot_accuracy

train(net)